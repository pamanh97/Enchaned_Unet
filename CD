import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchmetrics import Dice
from torchmetrics.segmentation import MeanIoU
import albumentations as A
import matplotlib.pyplot as plt
import numpy as np
import cv2
from tqdm import tqdm
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision.transforms.functional import to_tensor
import random

# Define directories for images and masks
image_folder = r'......................................................'
mask_folder = r'.......................................................'

# Dataset loading class
class RoadSegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir, transform=None, num_images=4000):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform  
        self.num_images = num_images
        all_image_list = sorted(os.listdir(image_dir))
        all_mask_list = sorted(os.listdir(mask_dir))
        assert len(all_image_list) == len(all_mask_list), "Mismatch between images and masks"
        self.image_list = all_image_list[:min(num_images, len(all_image_list))]
        self.mask_list = all_mask_list[:min(num_images, len(all_mask_list))]

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_list[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        image = to_tensor(image)  
        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)
        return image, mask

# Basic U-Net model
class UNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UNet, self).__init__()
        self.encoder1 = self.conv_block(in_channels, 64)
        self.encoder2 = self.conv_block(64, 128)
        self.encoder3 = self.conv_block(128, 256)
        self.encoder4 = self.conv_block(256, 512)
        
        self.bottleneck = self.conv_block(512, 1024)
        
        # Decoder with skip connections
        self.decoder4 = self.upconv_block(1024, 512)
        self.decoder3 = self.upconv_block(512, 256)
        self.decoder2 = self.upconv_block(256, 128)
        self.decoder1 = self.upconv_block(128, 64)
        
        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

    def upconv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(F.max_pool2d(enc1, 2))
        enc3 = self.encoder3(F.max_pool2d(enc2, 2))
        enc4 = self.encoder4(F.max_pool2d(enc3, 2))
        
        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))
        
        dec4 = self.decoder4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        
        dec3 = self.decoder3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        
        dec2 = self.decoder2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        
        dec1 = self.decoder1(dec2)
        
        return self.final_conv(dec1)

# Transformer Block for Hybrid UNet
class TransformerBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(TransformerBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Multi-head attention and Feed-Forward Network
        self.attn = nn.MultiheadAttention(embed_dim=out_channels, num_heads=8, batch_first=True)
        self.mlp = nn.Sequential(
            nn.Linear(out_channels, out_channels),
            nn.ReLU(inplace=True),
            nn.Linear(out_channels, out_channels)
        )

    def forward(self, x):
        batch_size, channels, height, width = x.size()

        # Flatten the spatial dimensions (height, width) into one dimension (seq_len)
        # Shape should be (batch_size, height * width, channels)
        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # (batch_size, height*width, channels)

        # Apply multi-head attention
        attn_output, _ = self.attn(x, x, x)

        # Reshape back to the original spatial dimensions
        x = attn_output.permute(0, 2, 1).view(batch_size, channels, height, width)
        
        # Apply Convolution layers with normalization
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        
        return x


# Hybrid U-Net with Transformer
class HybridUNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(HybridUNet, self).__init__()
        self.encoder1 = TransformerBlock(in_channels, 64)
        self.encoder2 = TransformerBlock(64, 128)
        self.encoder3 = TransformerBlock(128, 256)
        self.encoder4 = TransformerBlock(256, 512)
        
        self.bottleneck = TransformerBlock(512, 1024)
        
        # Decoder with skip connections
        self.decoder4 = self.upconv_block(1024, 512)
        self.decoder3 = self.upconv_block(512, 256)
        self.decoder2 = self.upconv_block(256, 128)
        self.decoder1 = self.upconv_block(128, 64)
        
        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def upconv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(F.max_pool2d(enc1, 2))
        enc3 = self.encoder3(F.max_pool2d(enc2, 2))
        enc4 = self.encoder4(F.max_pool2d(enc3, 2))
        
        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))
        
        dec4 = self.decoder4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        
        dec3 = self.decoder3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        
        dec2 = self.decoder2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        
        dec1 = self.decoder1(dec2)
        
        return self.final_conv(dec1)

# Loss Function
def combined_loss(output, target):
    bce_loss = nn.BCEWithLogitsLoss()(output, target)
    dice_loss = Dice()(output.sigmoid(), target)
    return bce_loss + dice_loss

# Optimizer and Scheduler
def get_optimizer(model, lr=0.001):
    return optim.Adam(model.parameters(), lr=lr)

def get_scheduler(optimizer):
    return ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

# Prepare DataLoader
train_augmentations = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),
    A.Rotate(limit=30, p=0.5),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),
])

# Dataset and DataLoader
full_dataset = RoadSegmentationDataset(image_folder, mask_folder, transform=train_augmentations, num_images=6993)

train_size = int(0.75 * len(full_dataset))
val_size = int(0.1 * len(full_dataset))
test_size = len(full_dataset) - train_size - val_size

train_indices, val_indices, test_indices = torch.utils.data.random_split(
    range(len(full_dataset)),
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)
)

train_loader = DataLoader(Subset(full_dataset, train_indices), batch_size=4, shuffle=True)
val_loader = DataLoader(Subset(full_dataset, val_indices), batch_size=4, shuffle=False)
test_loader = DataLoader(Subset(full_dataset, test_indices), batch_size=4, shuffle=False)

# Training function and visualization 
def train_and_evaluate(model, train_loader, val_loader, test_loader, device, num_epochs=50):
    optimizer = get_optimizer(model)
    scheduler = get_scheduler(optimizer)
    model = model.to(device)
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for images, masks in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [Train]"):
            images, masks = images.to(device), masks.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = combined_loss(outputs, masks)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)
        
        train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(train_loss)

        # Validation step
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(device), masks.to(device)
                outputs = model(images)
                loss = combined_loss(outputs, masks)
                val_loss += loss.item() * images.size(0)
        
        val_loss /= len(val_loader.dataset)
        val_losses.append(val_loss)

        scheduler.step(val_loss)

        print(f"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}")
    
    return train_losses, val_losses

# Visualizing the Results
def plot_results(train_losses, val_losses, model_name="Model"):
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.title(f"{model_name} Training vs Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.savefig(f"{model_name}_loss.png")
    plt.show()

# Evaluate the model
def evaluate_model(model, test_loader, device):
    model.eval()
    dice_metric = Dice(average='macro', num_classes=2).to(device)
    iou_metric = MeanIoU(num_classes=2).to(device)

    with torch.no_grad():
        for images, masks in test_loader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            outputs = torch.sigmoid(outputs) > 0.5
            dice_metric.update(outputs, masks)
            iou_metric.update(outputs, masks)

    avg_dice = dice_metric.compute()
    avg_iou = iou_metric.compute()
    print(f"Average Dice: {avg_dice:.4f}, Average IoU: {avg_iou:.4f}")
    return avg_dice, avg_iou

# Visualize the segmentation results
def visualize_results(model, test_loader, device, num_images=5):
    model.eval()
    test_samples = random.sample(list(test_loader), num_images)
    fig, axes = plt.subplots(num_images, 4, figsize=(20, 5 * num_images))
    for idx, (images, masks) in enumerate(test_samples):
        images, masks = images.to(device), masks.to(device)
        with torch.no_grad():
            outputs = model(images)
            outputs = (torch.sigmoid(outputs) > 0.5).float()
        original_image = images[0].cpu().permute(1, 2, 0).numpy()
        truth_mask = masks[0].squeeze(0).cpu().numpy()
        predicted_mask = outputs[0].squeeze(0).cpu().numpy()
        overlay = original_image.copy()
        pred_mask_normalized = cv2.normalize(predicted_mask, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        blue_overlay = np.zeros_like(original_image)
        blue_overlay[:, :, 2] = pred_mask_normalized
        alpha = 0.5
        overlay = cv2.addWeighted(original_image, 1 - alpha, blue_overlay, alpha, 0)
        axes[idx, 0].imshow(original_image)
        axes[idx, 0].set_title("Original Image")
        axes[idx, 0].axis("off")
        axes[idx, 1].imshow(overlay)
        axes[idx, 1].set_title("Prediction Overlay")
        axes[idx, 1].axis("off")
        axes[idx, 2].imshow(truth_mask, cmap="gray")
        axes[idx, 2].set_title("Ground Truth Mask")
        axes[idx, 2].axis("off")
        axes[idx, 3].imshow(predicted_mask, cmap="gray")
        axes[idx, 3].set_title("Predicted Mask")
        axes[idx, 3].axis("off")
    
    plt.tight_layout()
    plt.savefig("segmentation_results.png")
    plt.show()

# Initialize and train models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# HybridUNet
hybrid_model = HybridUNet(in_channels=3, out_channels=1).to(device)
train_losses_hybrid, val_losses_hybrid = train_and_evaluate(hybrid_model, train_loader, val_loader, test_loader, device, num_epochs=50)
plot_results(train_losses_hybrid, val_losses_hybrid, model_name="HybridUNet")

# U-Net
unet_model = UNet(in_channels=3, out_channels=1).to(device)
train_losses_unet, val_losses_unet = train_and_evaluate(unet_model, train_loader, val_loader, test_loader, device, num_epochs=50)
plot_results(train_losses_unet, val_losses_unet, model_name="U-Net")

# Evaluate models
print("Evaluating HybridUNet...")
avg_dice_hybrid, avg_iou_hybrid = evaluate_model(hybrid_model, test_loader, device)

print("Evaluating U-Net...")
avg_dice_unet, avg_iou_unet = evaluate_model(unet_model, test_loader, device)

# Visualize results for both models
print("Visualizing results for HybridUNet...")
visualize_results(hybrid_model, test_loader, device, num_images=5)

print("Visualizing results for U-Net...")
visualize_results(unet_model, test_loader, device, num_images=5)

# Save the evaluation results to files
with open("evaluation_results.txt", "w") as f:
    f.write(f"HybridUNet - Dice: {avg_dice_hybrid:.4f}, IoU: {avg_iou_hybrid:.4f}\n")
    f.write(f"U-Net - Dice: {avg_dice_unet:.4f}, IoU: {avg_iou_unet:.4f}\n")
    f.write("\nTraining Losses for HybridUNet\n")
    for epoch, loss in enumerate(train_losses_hybrid, 1):
        f.write(f"Epoch {epoch}: {loss:.4f}\n")
    f.write("\nValidation Losses for HybridUNet\n")
    for epoch, loss in enumerate(val_losses_hybrid, 1):
        f.write(f"Epoch {epoch}: {loss:.4f}\n")
    f.write("\nTraining Losses for U-Net\n")
    for epoch, loss in enumerate(train_losses_unet, 1):
        f.write(f"Epoch {epoch}: {loss:.4f}\n")
    f.write("\nValidation Losses for U-Net\n")
    for epoch, loss in enumerate(val_losses_unet, 1):
        f.write(f"Epoch {epoch}: {loss:.4f}\n")

print("Results saved in evaluation_results.txt")
